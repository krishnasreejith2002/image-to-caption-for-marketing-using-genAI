# ============================================
# üéØ Brand-Aware Captioning Fine-Tuning Notebook
# ============================================

!pip install transformers datasets accelerate torch -q

import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
import torch

# -----------------------------
# 1Ô∏è‚É£ Load Data
# -----------------------------
df = pd.read_csv("brand_tone_dataset.csv")
dataset = Dataset.from_pandas(df)

# -----------------------------
# 2Ô∏è‚É£ Load Model + Tokenizer
# -----------------------------
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# -----------------------------
# 3Ô∏è‚É£ Tokenize Dataset
# -----------------------------
def preprocess(examples):
    model_inputs = tokenizer(examples["input_text"], max_length=64, truncation=True)
    labels = tokenizer(examples["target_text"], max_length=64, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_ds = dataset.map(preprocess, batched=True)

# -----------------------------
# 4Ô∏è‚É£ Training Setup
# -----------------------------
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
training_args = TrainingArguments(
    output_dir="./brand_model",
    num_train_epochs=6,
    per_device_train_batch_size=2,
    learning_rate=5e-5,
    save_strategy="epoch",
    logging_dir="./logs",
    fp16=torch.cuda.is_available(),
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# -----------------------------
# 5Ô∏è‚É£ Train & Save Model
# -----------------------------
trainer.train()
trainer.save_model("./brand_model")
tokenizer.save_pretrained("./brand_model")

print("‚úÖ Fine-tuning complete! Model saved to ./brand_model")
